import os

# try:
#     import soundfile
# except:
#     os.system("pip install -r requirements.txt")

import gradio as gr
import json
import requests as req

script_path = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_path)
print(script_path)
from langchain.chat_models import ErnieBotChat
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

import time
import json
import requests as req
import utils

# flake8: noqa: E402
import os
import logging

logging.getLogger("numba").setLevel(logging.WARNING)
logging.getLogger("markdown_it").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("matplotlib").setLevel(logging.WARNING)

logging.basicConfig(
    level=logging.INFO, format="| %(name)s | %(levelname)s | %(message)s"
)

logger = logging.getLogger(__name__)

import torch
import utils
from infer import infer, latest_version, get_net_g, infer_multilang
import gradio as gr
import numpy as np
from config import config


device = config.webui_config.device
if device == "mps":
    os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"


xxx = """
ä½ æ˜¯ä¸€ä¸ªèŠå¤©åŠ©æ‰‹ï¼Œä½ éœ€è¦é€šè¿‡å®Œæˆè§’è‰²å¡‘é€ æ¥è·Ÿæˆ‘å¯¹è¯ã€‚
è¯·ä½ ä¸ºæˆ‘å¡‘é€ ä¸€ä¸ªè§’è‰²èƒŒæ™¯ï¼Œè§’è‰²äººç‰©ä¸ºã€ŠåŸç¥ã€‹çš„"{}"ï¼Œ

ã€å‚è€ƒæ ·ä¾‹ã€‘
<role_system_start>
ä½ æ­£åœ¨æ‰®æ¼”ã€Šçº¢æ¥¼æ¢¦ã€‹ä¸­çš„æ—é»›ç‰ï¼Œäººç‰©ä¿¡æ¯å¦‚ä¸‹ï¼š
äººç‰©åç§°ï¼šæ—é»›ç‰
æ€§åˆ«ï¼šå¥³æ€§
å¹´é¾„ï¼šé’å¹´
æ‘˜è¦ï¼šæ•…äº‹å‘ç”Ÿåœ¨æ¸…æœçš„å°å»ºç¤¾ä¼šï¼Œè´¾å®¶æ˜¯ä¸€ä¸ªè´µæ—å®¶åº­ï¼Œå®¶ä¸­æœ‰å¾ˆå¤šäººç‰©ï¼Œæ¯ä¸ªäººéƒ½æœ‰è‡ªå·±ç‹¬ç‰¹çš„ç‰¹ç‚¹å’Œæ•…äº‹ã€‚è´¾å®ç‰æ˜¯è´¾å®¶çš„å«¡ç”Ÿå­ï¼Œä»–èªæ˜ã€æœ‰æ‰åï¼Œä½†å´ä¸æƒ³èµ°å°å»ºä¼ ç»Ÿçš„é“è·¯ã€‚æ—é»›ç‰æ˜¯è´¾å®ç‰çš„è¡¨å¦¹ï¼Œå¥¹ç¾ä¸½ã€èªæ˜ã€æ•æ„Ÿï¼Œä½†å´åœ¨å®¶åº­çš„å‹åŠ›ä¸‹æ— æ³•å’Œè´¾å®ç‰åœ¨ä¸€èµ·ã€‚
è´¾å®¶çš„ç¹è£éšç€æ—¶ä»£çš„å˜åŒ–è€Œé€æ¸è¡°è´¥ï¼Œæœ€ç»ˆå®¶æ—ç ´äº§ï¼Œè®¸å¤šäººéƒ½å—åˆ°äº†æå¤§çš„å½±å“ã€‚è´¾å®ç‰å’Œæ—é»›ç‰çš„çˆ±æƒ…ä¹Ÿå› ä¸ºå®¶æ—çš„è¡°è´¥è€Œå—åˆ°äº†æå¤§çš„é˜»ç¢ï¼Œæœ€ç»ˆä¸¤äººæœªèƒ½åœ¨ä¸€èµ·ã€‚åŒæ—¶ï¼Œä¼—å¤šäººç‰©åœ¨æ‚²æ¬¢ç¦»åˆä¸­å±•ç°äº†å„è‡ªçš„äººæ€§ï¼Œæœ‰çš„ä¸ºäº†åˆ©ç›Šä¸æ‹©æ‰‹æ®µï¼Œæœ‰çš„ä¸ºäº†æ„Ÿæƒ…æ”¾å¼ƒäº†ä¸€åˆ‡ã€‚
è§’è‰²æ ‡ç­¾ï¼šå­¤åƒ»ã€æ¸…å†·
èº«ä»½/èŒä¸šï¼šè´¾æ¯å¤–å­™å¥³
äººç‰©å…³ç³»ï¼šä¸å®ç‰ä¸¤æƒ…ç›¸æ‚¦
ä¸‹é¢ï¼Œä½ å°†æ‰®æ¼”æ—é»›ç‰å’Œæˆ‘è¿›è¡Œå¯¹è¯ï¼Œæ°¸è¿œè®°ä½ä½ æ˜¯æ—é»›ç‰ï¼Œåœ¨å›å¤æ—¶æ³¨æ„ä¿æŒæ•æ„Ÿã€ç»†å¿ƒã€è‡ªå°Šå¿ƒå¼ºã€å¤šæ„å–„æ„Ÿã€å†°é›ªèªæ˜ã€æ‚Ÿæ€§æå¼ºã€ä»»æƒ…ä»»æ€§ã€è¿½æ±‚è‡ªç”±ã€çœŸè¯šå–„è‰¯ã€çš„é£æ ¼ã€‚
ä½ çš„å¼€åœºç™½ä¸ºï¼šä½ å¿ƒä¸­è‡ªç„¶æœ‰å¦¹å¦¹ï¼Œåªæ˜¯è§äº†å§å§ï¼Œå°±æŠŠå¦¹å¦¹å¿˜äº†....
<role_system_end>

ã€ä½ çš„å¡‘é€ è§’è‰²èƒŒæ™¯ã€‘ï¼š

"""


def generate_audio(
    slices,
    sdp_ratio,
    noise_scale,
    noise_scale_w,
    length_scale,
    speaker,
    language,
    skip_start=False,
    skip_end=False,
):
    audio_list = []
    # silence = np.zeros(hps.data.sampling_rate // 2, dtype=np.int16)
    with torch.no_grad():
        for idx, piece in enumerate(slices):
            skip_start = (idx != 0) and skip_start
            skip_end = (idx != len(slices) - 1) and skip_end
            audio = infer(
                piece,
                sdp_ratio=sdp_ratio,
                noise_scale=noise_scale,
                noise_scale_w=noise_scale_w,
                length_scale=length_scale,
                sid=speaker,
                language=language,
                hps=hps,
                net_g=net_g,
                device=device,
                skip_start=skip_start,
                skip_end=skip_end,
            )
            audio16bit = gr.processing_utils.convert_to_16_bit_wav(audio)
            audio_list.append(audio16bit)
            # audio_list.append(silence)  # å°†é™éŸ³æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    return audio_list


def tts_fn(speaker, chatbot, sdp, noise, noisew, length):
    text = chatbot[-1][1]
    audio_list = []

    length = (100 - length) / 100

    audio_list.extend(
        generate_audio(
            text.split("|"),
            sdp,
            noise,
            noisew,
            length,
            speaker,
            language="ZH",
        )
    )

    audio_concat = np.concatenate(audio_list)
    return (hps.data.sampling_rate, audio_concat)


def g_bot(
    speaker,
    chatbot=None,
    history_state=ConversationBufferMemory(),
    temperature=None,
    llm_model=None,
):
    prompt = xxx.format(speaker)
    try:
        if llm_model is None:
            llm_model = init_model(temperature)
        output = init_base_chain(llm_model, history=history_state, user_question=prompt)
    except Exception as e:
        raise e
    chatbot = [[None, output.split("ä½ çš„å¼€åœºç™½ä¸ºï¼š")[1].split("\n")[0]]]

    return chatbot, output


def get_spk(spklist):
    resp = req.get(url=f"{spklist}/spklist/spks.json")
    data = json.loads(resp.text)
    return data


def search_speaker(search_value):
    for s in speakers:
        if search_value == s:
            return s
    for s in speakers:
        if search_value in s:
            return s


###llm
ernie_client_id = "96BMhQM5simx6R97yDl483Zm"
ernie_client_secret = "9e05mDOjHoyXD7Sb9GA1l420uaZ6vGMo"


def init_model(temperature):

    llm_model = ErnieBotChat(
        ernie_client_id=ernie_client_id,
        ernie_client_secret=ernie_client_secret,
        model_name="ERNIE-Bot-4",
        temperature=temperature,
        top_p=0.4,
    )
    return llm_model


def init_base_chain(llm_model, history, user_question=None):
    chain = ConversationChain(
        llm=llm_model,
        verbose=True,
        memory=history,
    )
    try:
        output = chain.run(user_question)
    except Exception as e:
        raise e
    return output


###gradio
block = gr.Blocks(css="footer {visibility: hidden}", title="è§’è‰²æ‰®æ¼”å¯¹è¯")
hps = utils.get_hparams_from_file(config.webui_config.config_path)
version = hps.version if hasattr(hps, "version") else latest_version
net_g = get_net_g(
    model_path=config.webui_config.model, version=version, device=device, hps=hps
)
with block:
    gr.HTML("<center>" "<h1>ğŸ’•ğŸ¶ ã€Œå£°ä¸´å…¶å¢ƒã€ X è§’è‰²æ‰®æ¼” </h1>" "</center>")
    gr.Markdown("## <center>âš¡ å¿«é€Ÿä½“éªŒç‰ˆï¼Œé€¼çœŸçš„è§’è‰²å£°éŸ³ï¼Œè®©ä½ æ²‰æµ¸å…¶ä¸­ã€‚</center>")
    gr.Markdown(
        "### <center>å¦‚æœæœªç‚¹å‡»â€œç”Ÿæˆâ€æŒ‰é’®ï¼Œå°†ä¼šè¿›å…¥æ™®é€šæœºå™¨äººå°åŠ©æ‰‹å¯¹è¯æ¨¡å¼ã€‚ç”Ÿæˆè§’è‰²ä¼šå’Œç¬¬ä¸€æ¬¡å¯¹è¯éœ€è¦æ€»æ—¶é—´10-20sï¼Œåç»­èŠå¤©åŸºæœ¬1-2sä»¥åŠå®æ—¶ç”Ÿæˆè¯­éŸ³ã€‚ğŸ˜ŠğŸ­</center>"
    )

    speaker_ids = hps.data.spk2id
    speakers = list(speaker_ids.keys())
    history = ConversationBufferMemory()  # å†å²è®°å½•
    history_state = gr.State(history)  # å†å²è®°å½•çš„çŠ¶æ€
    llm_model_state = gr.State()  # llmæ¨¡å‹çš„çŠ¶æ€
    trash = gr.State()  # åƒåœ¾æ¡¶
    with gr.Row():
        # è®¾ç½®è¡Œ

        with gr.Column(scale=1.8):
            with gr.Accordion("æ¨¡å‹é…ç½®", open=True):
                with gr.Row():
                    speaker = gr.Dropdown(
                        choices=speakers, value=speakers[0], label="è§’è‰²"
                    )
                    search = gr.Textbox(label="æœç´¢è§’è‰²", lines=1)
                    with gr.Column():
                        with gr.Row():
                            btn_ensure = gr.Button(value="ç”Ÿæˆ")
                            btn2 = gr.Button(value="æœç´¢")
                        with gr.Row():
                            text = gr.TextArea(
                                label="è§’è‰²èƒŒæ™¯",
                                placeholder="é€‰æ‹©è§’è‰²ï¼ŒAIç”Ÿæˆè§’è‰²èƒŒæ™¯......",
                                lines=10,
                                interactive=True,
                            )

                with gr.Column():
                    with gr.Row():
                        sdp_ratio = gr.Slider(
                            minimum=0,
                            maximum=1,
                            value=0.2,
                            step=0.1,
                            label="SDP/DP æ··åˆæ¯”",
                        )
                        noise_scale = gr.Slider(
                            minimum=0.1, maximum=2, value=0.6, step=0.1, label="æ„Ÿæƒ…"
                        )
                    with gr.Row():
                        noise_scale_w = gr.Slider(
                            minimum=0.1,
                            maximum=2,
                            value=0.8,
                            step=0.1,
                            label="éŸ³ç´ é•¿åº¦",
                        )
                        length_scale = gr.Slider(
                            minimum=-99, maximum=99, value=0, step=0.1, label="è¯­é€Ÿ(%)"
                        )
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.7,
                    step=0.1,
                    label="temperature",
                    interactive=True,
                )

        with gr.Column(scale=4):

            chatbot = gr.Chatbot(label="èŠå¤©å¯¹è¯æ¡†", lines=80)
            with gr.Row():
                message = gr.Textbox(
                    label="åœ¨æ­¤å¤„å¡«å†™ä½ æƒ³å¯¹æˆ‘è¯´çš„è¯",
                    placeholder="æˆ‘æœ‰å¾ˆå¤šè¯æƒ³è·Ÿä½ è¯´......",
                    lines=2,
                )
            with gr.Row():
                audio_output = gr.Audio(label="è¾“å‡ºéŸ³é¢‘")
            with gr.Row():
                submit = gr.Button("å‘é€", variant="primary")
                # åˆ·æ–°
                clear = gr.Button("é‡ç½®", variant="secondary")

            def clear_():
                chatbot = []
                history_state = ConversationBufferMemory()
                return (
                    "",
                    chatbot,
                    history_state,
                    "å·²é‡ç½®æˆåŠŸï¼Œè¯·é‡æ–°å¼€å§‹é€‰æ‹©è§’è‰²ç”ŸæˆèƒŒæ™¯è§’è‰²......",
                )

            def user(user_message, history):
                return "", history + [[user_message, None]]

            def bot(
                user_message,
                chatbot=None,
                history_state=ConversationBufferMemory(),
                temperature=None,
                llm_model=None,
            ):
                try:
                    user_message = chatbot[-1][0]
                    if llm_model is None:
                        llm_model = init_model(temperature)
                    output = init_base_chain(
                        llm_model, history=history_state, user_question=user_message
                    )
                except Exception as e:
                    raise e
                chatbot[-1][1] = ""
                for character in output:
                    chatbot[-1][1] += character
                    time.sleep(0.03)
                    yield chatbot
                return chatbot

    btn2.click(search_speaker, inputs=[search], outputs=[speaker])

    btn_ensure.click(
        g_bot,
        inputs=[speaker, chatbot, history_state, temperature, llm_model_state],
        outputs=[chatbot, text],
        queue=False,
    ).then(
        tts_fn,
        inputs=[speaker, chatbot, sdp_ratio, noise_scale, noise_scale_w, length_scale],
        outputs=[audio_output],
    )

    # å›è½¦
    message.submit(user, [message, chatbot], [message, chatbot], queue=False).then(
        bot, [message, chatbot, history_state, temperature, llm_model_state], [chatbot]
    ).then(
        tts_fn,
        inputs=[speaker, chatbot, sdp_ratio, noise_scale, noise_scale_w, length_scale],
        outputs=[audio_output],
    )
    # åˆ·æ–°æŒ‰é’®
    clear.click(clear_, inputs=[], outputs=[message, chatbot, history_state, text])
    # sendæŒ‰é’®
    submit.click(user, [message, chatbot], [message, chatbot], queue=False).then(
        bot, [message, chatbot, history_state, temperature, llm_model_state], [chatbot]
    ).then(
        tts_fn,
        inputs=[speaker, chatbot, sdp_ratio, noise_scale, noise_scale_w, length_scale],
        outputs=[audio_output],
    )
    gr.Markdown(
        "### <right>æ›´å¤šç²¾å½©éŸ³é¢‘åº”ç”¨ï¼Œæ­£åœ¨æŒç»­æ›´æ–°ï½è”ç³»ä½œè€…ï¼šluyao15@baidu.com ğŸ’•</right>"
    )


# å¯åŠ¨å‚æ•°
block.queue(concurrency_count=32).launch(
    debug=False,
    # server_name=config['block']['server_name'],
    # server_port=config['block']['server_port'],
    server_name="0.0.0.0",
    server_port=8906,
    share=True,
)
